{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AgHuDndvZVj4",
    "outputId": "722c9c96-805a-45e3-bb87-2e579e62c5fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.2.11\n",
      "  Downloading langchain-0.2.11-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-community==0.2.10\n",
      "  Downloading langchain_community-0.2.10-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain-text-splitters==0.2.2\n",
      "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting langchain-groq==0.1.6\n",
      "  Downloading langchain_groq-0.1.6-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting transformers==4.43.2\n",
      "  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentence-transformers==3.0.1\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting unstructured==0.15.0\n",
      "  Downloading unstructured-0.15.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting PyPDF2==3.0.1\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting python-dotenv==1.0.0\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting streamlit==1.18.1\n",
      "  Downloading streamlit-1.18.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting openai==0.27.6\n",
      "  Downloading openai-0.27.6-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting faiss-cpu==1.7.4\n",
      "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting altair==4\n",
      "  Downloading altair-4.0.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tiktoken==0.4.0\n",
      "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (3.11.9)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (4.0.3)\n",
      "Collecting langchain-core<0.3.0,>=0.2.23 (from langchain==0.2.11)\n",
      "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (2.10.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.2.11) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain==0.2.11)\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.2.10)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting groq<1,>=0.4.1 (from langchain-groq==0.1.6)\n",
      "  Downloading groq-0.13.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (0.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (2024.9.11)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.43.2)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.2) (4.66.6)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (1.5.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (1.13.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==3.0.1) (11.0.0)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (5.2.0)\n",
      "Collecting filetype (from unstructured==0.15.0)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured==0.15.0)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (5.3.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (3.9.1)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (0.9.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (4.12.3)\n",
      "Collecting emoji (from unstructured==0.15.0)\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting python-iso639 (from unstructured==0.15.0)\n",
      "  Downloading python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured==0.15.0)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting rapidfuzz (from unstructured==0.15.0)\n",
      "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting backoff (from unstructured==0.15.0)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (4.12.2)\n",
      "Collecting unstructured-client (from unstructured==0.15.0)\n",
      "  Downloading unstructured_client-0.28.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (1.17.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured==0.15.0) (5.9.5)\n",
      "Requirement already satisfied: blinker>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (1.9.0)\n",
      "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (5.5.0)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (8.5.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (2.2.2)\n",
      "Collecting protobuf<4,>=3.12 (from streamlit==1.18.1)\n",
      "  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (17.0.0)\n",
      "Collecting pympler>=0.9 (from streamlit==1.18.1)\n",
      "  Downloading Pympler-1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (2.8.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (13.9.4)\n",
      "Collecting semver (from streamlit==1.18.1)\n",
      "  Downloading semver-3.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (0.10.2)\n",
      "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (5.2)\n",
      "Collecting validators>=0.2 (from streamlit==1.18.1)\n",
      "  Downloading validators-0.34.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: gitpython!=3.1.19 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (3.1.43)\n",
      "Collecting pydeck>=0.1.dev5 (from streamlit==1.18.1)\n",
      "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit==1.18.1) (6.3.3)\n",
      "Collecting watchdog (from streamlit==1.18.1)\n",
      "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair==4) (0.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair==4) (3.1.4)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from altair==4) (4.23.0)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair==4) (0.12.1)\n",
      "Collecting onnx (from unstructured[pdf]==0.15.0)\n",
      "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting pdf2image (from unstructured[pdf]==0.15.0)\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pdfminer.six (from unstructured[pdf]==0.15.0)\n",
      "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pikepdf (from unstructured[pdf]==0.15.0)\n",
      "  Downloading pikepdf-9.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.2 kB)\n",
      "Collecting pillow-heif (from unstructured[pdf]==0.15.0)\n",
      "  Downloading pillow_heif-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.8 kB)\n",
      "Collecting pypdf (from unstructured[pdf]==0.15.0)\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting pytesseract (from unstructured[pdf]==0.15.0)\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-cloud-vision (from unstructured[pdf]==0.15.0)\n",
      "  Downloading google_cloud_vision-3.8.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting effdet (from unstructured[pdf]==0.15.0)\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting unstructured-inference==0.7.36 (from unstructured[pdf]==0.15.0)\n",
      "  Downloading unstructured_inference-0.7.36-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[pdf]==0.15.0)\n",
      "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting layoutparser (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
      "  Downloading layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting python-multipart (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
      "  Downloading python_multipart-0.0.19-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (4.10.0.84)\n",
      "Collecting onnxruntime>=1.17.0 (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
      "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (3.8.0)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (1.0.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.11) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
      "  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19->streamlit==1.18.1) (4.0.11)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.6) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.6) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.6) (0.28.0)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq==0.1.6) (1.3.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.2) (2024.10.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->streamlit==1.18.1) (3.21.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain==0.2.11) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.11) (1.0.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->streamlit==1.18.1) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->streamlit==1.18.1) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.2.11) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.2.11) (2.27.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair==4) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->streamlit==1.18.1) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.11) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.11) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.11) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.2.11) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->streamlit==1.18.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->streamlit==1.18.1) (2.18.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.11) (3.1.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (3.4.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers==3.0.1) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers==3.0.1) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured==0.15.0) (2.6)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]==0.15.0) (0.20.1+cu121)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->unstructured[pdf]==0.15.0) (2.0.8)\n",
      "Collecting omegaconf>=2.0 (from effdet->unstructured[pdf]==0.15.0)\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]==0.15.0) (2.19.2)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]==0.15.0) (2.27.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision->unstructured[pdf]==0.15.0) (1.25.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->altair==4) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->altair==4) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->altair==4) (0.22.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.15.0) (1.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured[pdf]==0.15.0) (43.0.3)\n",
      "Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from pikepdf->unstructured[pdf]==0.15.0) (1.2.15)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==3.0.1) (3.5.0)\n",
      "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured==0.15.0)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured==0.15.0) (0.2.0)\n",
      "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured==0.15.0)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured==0.15.0) (1.6.0)\n",
      "Collecting pydantic<3,>=1 (from langchain==0.2.11)\n",
      "  Downloading pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.4/149.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic-core==2.23.4 (from pydantic<3,>=1->langchain==0.2.11)\n",
      "  Downloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq==0.1.6) (1.2.2)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured[pdf]==0.15.0) (1.17.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19->streamlit==1.18.1) (5.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]==0.15.0) (1.66.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]==0.15.0) (1.68.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]==0.15.0) (1.62.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]==0.15.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]==0.15.0) (4.9)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.1.6) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq==0.1.6) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain==0.2.11) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->streamlit==1.18.1) (0.1.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[pdf]==0.15.0)\n",
      "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting coloredlogs (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (24.3.25)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0) (3.2.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.10)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting iopath (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
      "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting pdfplumber (from layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[pdf]==0.15.0) (2.22)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[pdf]==0.15.0)\n",
      "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.62.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.62.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.62.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.61.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.60.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.60.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.60.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.59.5-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.59.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.59.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.59.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.58.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.58.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.57.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "  Downloading grpcio_status-1.56.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.56.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.55.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.54.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.54.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.54.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.53.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.53.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.53.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.51.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.51.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.50.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.49.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "  Downloading grpcio_status-1.48.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision->unstructured[pdf]==0.15.0) (0.6.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
      "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pdfminer.six (from unstructured[pdf]==0.15.0)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.7.36->unstructured[pdf]==0.15.0)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
      "Downloading langchain_groq-0.1.6-py3-none-any.whl (14 kB)\n",
      "Downloading transformers-4.43.2-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured-0.15.0-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Downloading streamlit-1.18.1-py2.py3-none-any.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading altair-4.0.0-py2.py3-none-any.whl (709 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m709.0/709.0 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured_inference-0.7.36-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading groq-0.13.0-py3-none-any.whl (108 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Pympler-1.1-py3-none-any.whl (165 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.8/165.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading validators-0.34.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_cloud_vision-3.8.1-py2.py3-none-any.whl (486 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.9/486.9 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pikepdf-9.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow_heif-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
      "Downloading unstructured_client-0.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.9/434.9 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.23.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Downloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m107.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.19-py3-none-any.whl (24 kB)\n",
      "Downloading grpcio_status-1.48.2-py3-none-any.whl (14 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
      "Building wheels for collected packages: langdetect, antlr4-python3-runtime, iopath\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=8cf854721666da39ebbee72a5911f5a38dc34017000aeeb244f5e954a258c9b5\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=2010630f61a36f409437a0cd557cc399fa0d713c1776663dcd284b8bf75d95f0\n",
      "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31528 sha256=cebbd39287155540d6d0c82bc66ea29bb9eae8f45d9054e4ceeb3e028c6e35b5\n",
      "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
      "Successfully built langdetect antlr4-python3-runtime iopath\n",
      "Installing collected packages: filetype, faiss-cpu, antlr4-python3-runtime, watchdog, validators, unstructured.pytesseract, tenacity, semver, rapidfuzz, python-multipart, python-magic, python-iso639, python-dotenv, pytesseract, pypdfium2, PyPDF2, pypdf, pympler, pydantic-core, protobuf, portalocker, pillow-heif, pdf2image, omegaconf, mypy-extensions, marshmallow, langdetect, jsonpath-python, humanfriendly, emoji, backoff, aiofiles, typing-inspect, tiktoken, pydeck, pydantic, pikepdf, onnx, iopath, coloredlogs, unstructured-client, tokenizers, pdfminer.six, onnxruntime, grpcio-status, groq, dataclasses-json, unstructured, transformers, pdfplumber, openai, langchain-core, altair, streamlit, sentence-transformers, layoutparser, langchain-text-splitters, langchain-groq, google-cloud-vision, effdet, unstructured-inference, langchain, langchain-community\n",
      "  Attempting uninstall: tenacity\n",
      "    Found existing installation: tenacity 9.0.0\n",
      "    Uninstalling tenacity-9.0.0:\n",
      "      Successfully uninstalled tenacity-9.0.0\n",
      "  Attempting uninstall: pydantic-core\n",
      "    Found existing installation: pydantic_core 2.27.1\n",
      "    Uninstalling pydantic_core-2.27.1:\n",
      "      Successfully uninstalled pydantic_core-2.27.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.5\n",
      "    Uninstalling protobuf-4.25.5:\n",
      "      Successfully uninstalled protobuf-4.25.5\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.2\n",
      "    Uninstalling pydantic-2.10.2:\n",
      "      Successfully uninstalled pydantic-2.10.2\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: grpcio-status\n",
      "    Found existing installation: grpcio-status 1.62.3\n",
      "    Uninstalling grpcio-status-1.62.3:\n",
      "      Successfully uninstalled grpcio-status-1.62.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.3\n",
      "    Uninstalling transformers-4.46.3:\n",
      "      Successfully uninstalled transformers-4.46.3\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.54.5\n",
      "    Uninstalling openai-1.54.5:\n",
      "      Successfully uninstalled openai-1.54.5\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.21\n",
      "    Uninstalling langchain-core-0.3.21:\n",
      "      Successfully uninstalled langchain-core-0.3.21\n",
      "  Attempting uninstall: altair\n",
      "    Found existing installation: altair 4.2.2\n",
      "    Uninstalling altair-4.2.2:\n",
      "      Successfully uninstalled altair-4.2.2\n",
      "  Attempting uninstall: sentence-transformers\n",
      "    Found existing installation: sentence-transformers 3.2.1\n",
      "    Uninstalling sentence-transformers-3.2.1:\n",
      "      Successfully uninstalled sentence-transformers-3.2.1\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.2\n",
      "    Uninstalling langchain-text-splitters-0.3.2:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.2\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.9\n",
      "    Uninstalling langchain-0.3.9:\n",
      "      Successfully uninstalled langchain-0.3.9\n",
      "Successfully installed PyPDF2-3.0.1 aiofiles-24.1.0 altair-4.0.0 antlr4-python3-runtime-4.9.3 backoff-2.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 effdet-0.4.1 emoji-2.14.0 faiss-cpu-1.7.4 filetype-1.2.0 google-cloud-vision-3.8.1 groq-0.13.0 grpcio-status-1.48.2 humanfriendly-10.0 iopath-0.1.10 jsonpath-python-1.0.6 langchain-0.2.11 langchain-community-0.2.10 langchain-core-0.2.43 langchain-groq-0.1.6 langchain-text-splitters-0.2.2 langdetect-1.0.9 layoutparser-0.3.4 marshmallow-3.23.1 mypy-extensions-1.0.0 omegaconf-2.3.0 onnx-1.17.0 onnxruntime-1.20.1 openai-0.27.6 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pikepdf-9.4.2 pillow-heif-0.21.0 portalocker-3.0.0 protobuf-3.20.3 pydantic-2.9.2 pydantic-core-2.23.4 pydeck-0.9.1 pympler-1.1 pypdf-5.1.0 pypdfium2-4.30.0 pytesseract-0.3.13 python-dotenv-1.0.0 python-iso639-2024.10.22 python-magic-0.4.27 python-multipart-0.0.19 rapidfuzz-3.10.1 semver-3.0.2 sentence-transformers-3.0.1 streamlit-1.18.1 tenacity-8.5.0 tiktoken-0.4.0 tokenizers-0.19.1 transformers-4.43.2 typing-inspect-0.9.0 unstructured-0.15.0 unstructured-client-0.28.1 unstructured-inference-0.7.36 unstructured.pytesseract-0.3.13 validators-0.34.0 watchdog-6.0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "97a498bf2cc94c18b7df43a7456e66db",
       "pip_warning": {
        "packages": [
         "google",
         "pydevd_plugins"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install langchain==0.2.11 langchain-community==0.2.10 langchain-text-splitters==0.2.2 langchain-groq==0.1.6 transformers==4.43.2 sentence-transformers==3.0.1 unstructured==0.15.0 unstructured[pdf]==0.15.0 PyPDF2==3.0.1 python-dotenv==1.0.0 streamlit==1.18.1 openai==0.27.6 faiss-cpu==1.7.4 altair==4 tiktoken==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6VOh6CilkGZT"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'streamlit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfReader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'streamlit'"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings,HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Xf1l1rsZWeW",
    "outputId": "f919c0df-defc-4ee6-9a2e-4c81085b6dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "#Create VectorDB Function\n",
    "import streamlit as st\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings,HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import RetrievalQA\n",
    "# '''\n",
    "# This function create an vectorDB which save all the data you want to ask about in a database using vectors this speedup search and help LLM understand the data\n",
    "# you can adjest the type of embedding for olamaa embedding or openai or hugging face embedding ,etc\n",
    "# and save vectordb in a folder called VectorDB\n",
    "# pdf path and embedding as parameters\n",
    "# return none\n",
    "# '''\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "\n",
    "def Create_vectordb(PDF,embeddings):\n",
    "  # loading the document\n",
    "  text = \"\"\n",
    "  pdf_reader = PdfReader(PDF)\n",
    "  for page in pdf_reader.pages:\n",
    "    text += page.extract_text()\n",
    "\n",
    "  # splitting into text chunks\n",
    "  # text_splitter = CharacterTextSplitter(\n",
    "  #     separator=\"\\n\",\n",
    "  #     chunk_size=2000,\n",
    "  #     chunk_overlap=400,\n",
    "  #     length_function=len\n",
    "  # )\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "\n",
    "  texts = text_splitter.split_text(text)\n",
    "  vectordb = FAISS.from_texts(texts=texts\n",
    "                              ,embedding=embeddings)\n",
    "  return vectordb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lwugQTUwQpCz",
    "outputId": "0477374c-0305-4450-8760-0b128797f50b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py -a\n",
    "#load VectorDB Function\n",
    "# '''\n",
    "# Load VectorDB\n",
    "\n",
    "# parameters\n",
    "# Folder path Location of the vectorDB in disk\n",
    "# Embeddings that embedding that was used while creating the vectordb\n",
    "\n",
    "# return\n",
    "# VectorDB variable\n",
    "# '''\n",
    "def Load_vectordb(Folder_Path,embeddings):\n",
    "  return FAISS.load_local(folder_path=Folder_Path,embeddings=embeddings,allow_dangerous_deserialization=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UvLV4ozCRClg",
    "outputId": "a7865fdf-09b9-42df-dfc9-3c68a314a903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py -a\n",
    "#save vectorDB function\n",
    "# '''\n",
    "# save VectorDB\n",
    "\n",
    "# parameters\n",
    "# VectorDB takes the vectorDB you want to save as a variable\n",
    "# Folder path Location of the vectorDB in disk\n",
    "\n",
    "\n",
    "# return\n",
    "# none\n",
    "# '''\n",
    "def save_vectordb(vectordb,Folder_Path):\n",
    "   vectordb.save_local(folder_path=Folder_Path)\n",
    "   print(\"save successful\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tllVSb3GR4NG",
    "outputId": "f3fa6181-9360-4dec-8c4b-31b92afec107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py -a\n",
    "#update VectorDB function\n",
    "# '''\n",
    "# Updates the vectorDB with new data\n",
    "# size of the chunk is 2000 Characters and 400 overlaps\n",
    "\n",
    "# available data Types (PDF,Website from wikiPedia)\n",
    "\n",
    "# parameters\n",
    "# Folder_path: Location of the vectorDB in disk\n",
    "# Embeddings: that embedding that was used while creating the vectordb\n",
    "# Data: path PDF or the link of website\n",
    "# Type:1--> PDF   2--> website link\n",
    "# '''\n",
    "def Update_VectorDB(Folder_Path,embeddings,Data,Type):\n",
    "  vectordb = Load_vectordb(Folder_Path,embeddings)\n",
    "\n",
    "  # splitting into text chunks\n",
    "  if Type== 1:\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len)\n",
    "    text = \"\"\n",
    "    pdf_reader = PdfReader(Data)\n",
    "    for page in pdf_reader.pages:\n",
    "      text += page.extract_text()\n",
    "    texts = text_splitter.split_text(text)\n",
    "    vectordb.add_texts(texts)\n",
    "    print(\"VectorDB updated\")\n",
    "  if Type == 2:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "    loader = WebBaseLoader(web_paths=(Data,),\n",
    "                          bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                              class_=(\"mw-body\"))))\n",
    "    text_documents = loader.load()\n",
    "\n",
    "    # Convert the list to an iterable\n",
    "    documents = text_splitter.split_documents(text_documents)\n",
    "    vectordb.add_documents(documents)\n",
    "  return vectordb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WOfaddalXMw",
    "outputId": "ad4b5d69-2e25-4029-c146-2007ce16ceb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py -a\n",
    "#function to load_LLM using Groq\n",
    "# '''\n",
    "# deploy the LLM model using groq api\n",
    "\n",
    "# parameters\n",
    "# vectordb take the vectordb where all the data in stored\n",
    "# model_name the type of LLM you want to use in groq api here is link of all available models\n",
    "# '''\n",
    "#add groq key\n",
    "def load_LLM_groq(vectordb,Model_name):\n",
    "  retriever = vectordb.as_retriever()\n",
    "  llm = ChatGroq(\n",
    "    model=Model_name,\n",
    "    temperature=0)\n",
    "  qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True)\n",
    "  return qa_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDdZzlPDoAhk"
   },
   "outputs": [],
   "source": [
    "def load_LLM_groq(vectordb,Model_name):\n",
    "  retriever = vectordb.as_retriever()\n",
    "  llm = ChatGroq(\n",
    "    model=Model_name,\n",
    "    temperature=0)\n",
    "  qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True)\n",
    "  return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H--FCJZL_jaC",
    "outputId": "c9f80476-fcb6-4fda-bf40-6755d26383f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py -a\n",
    "#function to Load_LLM using OpenAI\n",
    "# '''\n",
    "# deploy the LLM model using OpenAI api\n",
    "\n",
    "# parameters\n",
    "# vectordb take the vectordb where all the data in stored\n",
    "# model_name the type of LLM you want to use in OpenAI api here is link of all available models\n",
    "# '''\n",
    "# Add OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
    "def load_LLM_openai(vectordb, model_name=\"gpt-4\"):\n",
    "    retriever = vectordb.as_retriever()\n",
    "\n",
    "    # Initialize OpenAI model\n",
    "    llm = OpenAI(\n",
    "        model_name=model_name,  # Example: \"gpt-4o\" or \"gpt-3.5-turbo\"\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    # Set up RetrievalQA chain with OpenAI model\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # Use 'stuff', 'map_reduce', etc. as needed\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return qa_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rd_n6MzJwnyu",
    "outputId": "a662b7f6-9455-4097-dfb7-bff0749b025b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-f7b259c2029c>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda\n",
      "2024-12-05 21:06:03.009 INFO    sentence_transformers.SentenceTransformer: Use pytorch device_name: cuda\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "2024-12-05 21:06:03.013 INFO    sentence_transformers.SentenceTransformer: Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "2024-12-05 21:06:06.975 INFO    faiss.loader: Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "2024-12-05 21:06:06.998 INFO    faiss.loader: Successfully loaded faiss with AVX2 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vectorDB\n",
      "save successful\n",
      "VectorDB updated\n",
      "VectorDB updated\n",
      "VectorDB updated\n",
      "VectorDB updated\n",
      "VectorDB updated\n",
      "VectorDB updated\n",
      "save successful\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "\n",
    "def Create_vectordb(PDF,embeddings):\n",
    "  # loading the document\n",
    "  text = \"\"\n",
    "  pdf_reader = PdfReader(PDF)\n",
    "  for page in pdf_reader.pages:\n",
    "    text += page.extract_text()\n",
    "\n",
    "  # splitting into text chunks\n",
    "  # text_splitter = CharacterTextSplitter(\n",
    "  #     separator=\"\\n\",\n",
    "  #     chunk_size=2000,\n",
    "  #     chunk_overlap=400,\n",
    "  #     length_function=len\n",
    "  # )\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "\n",
    "  texts = text_splitter.split_text(text)\n",
    "  vectordb = FAISS.from_texts(texts=texts\n",
    "                              ,embedding=embeddings)\n",
    "  return vectordb\n",
    "\n",
    "\n",
    "def Update_VectorDB(Folder_Path,embeddings,Data,Type):\n",
    "  vectordb = Load_vectordb(Folder_Path,embeddings)\n",
    "\n",
    "  # splitting into text chunks\n",
    "  if Type== 1:\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len)\n",
    "    text = \"\"\n",
    "    pdf_reader = PdfReader(Data)\n",
    "    for page in pdf_reader.pages:\n",
    "      text += page.extract_text()\n",
    "    texts = text_splitter.split_text(text)\n",
    "    vectordb.add_texts(texts)\n",
    "    print(\"VectorDB updated\")\n",
    "  if Type == 2:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "    loader = WebBaseLoader(web_paths=(Data,),\n",
    "                          bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                              class_=(\"mw-body\"))))\n",
    "    text_documents = loader.load()\n",
    "\n",
    "    # Convert the list to an iterable\n",
    "    documents = text_splitter.split_documents(text_documents)\n",
    "    vectordb.add_documents(documents)\n",
    "    print(\"VectorDB updated\")\n",
    "  return vectordb\n",
    "\n",
    "def save_vectordb(vectordb,Folder_Path):\n",
    "   vectordb.save_local(folder_path=Folder_Path)\n",
    "   print(\"save successful\")\n",
    "def Load_vectordb(Folder_Path,embeddings):\n",
    "   return FAISS.load_local(folder_path=Folder_Path,embeddings=embeddings,allow_dangerous_deserialization=True)\n",
    "\n",
    "\n",
    "\n",
    "vectordb = Create_vectordb(\"/content/Ancient History of Egypt.pdf\",embeddings)\n",
    "print(\"created vectorDB\")\n",
    "save_vectordb(vectordb,\"VectorDB\")\n",
    "vectordb = Load_vectordb(\"VectorDB\",embeddings)\n",
    "vectordb = Update_VectorDB(\"VectorDB\",embeddings,\"https://en.wikipedia.org/wiki/History_of_Egypt\",2)\n",
    "vectordb = Update_VectorDB(\"VectorDB\",embeddings,\"https://en.wikipedia.org/wiki/State_of_Palestine\",2)\n",
    "vectordb = Update_VectorDB(\"VectorDB\",embeddings,\"https://en.wikipedia.org/wiki/History_of_Palestine\",2)\n",
    "vectordb = Update_VectorDB(\"VectorDB\",embeddings,\"https://en.wikipedia.org/wiki/Arabic_history\",2)\n",
    "vectordb = Update_VectorDB(\"VectorDB\",embeddings,\"https://en.wikipedia.org/wiki/Arabic_culture\",2)\n",
    "vectordb = Update_VectorDB(\"VectorDB\",embeddings,\"https://en.wikipedia.org/wiki/Arabic_religion\",2)\n",
    "save_vectordb(vectordb,\"VectorDB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JltI6Pi3VNcT"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmt4zsBQ8k4P",
    "outputId": "37c819c5-5586-441e-b927-525d43c075a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Arabic_religion'}, page_content='Other reasons this message may be displayed:\\n\\nIf a page was recently created here, it may not be visible yet because of a delay in updating the database; wait a few minutes or try the purge function.\\nTitles on Wikipedia are case sensitive except for the first character; please check alternative capitalizations and consider adding a redirect here to the correct title.\\nIf the page has been deleted, check the deletion log, and see Why was the page I created deleted?\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/wiki/Arabic_religion\"'), Document(metadata={'source': 'https://en.wikipedia.org/wiki/Arabic_religion'}, page_content='Arabic religion\\n\\n\\n\\nAdd languages\\n\\n\\n\\n\\t\\t\\t\\t\\t\\tPage contents not supported in other languages.\\n\\t\\t\\t\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereUpload fileSpecial pagesPrintable versionPage informationGet shortened URLDownload QR code\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\n\\n\\nLook for Arabic religion on one of Wikipedia\\'s sister projects:\\n\\n\\n\\n\\nWiktionary (dictionary)\\n\\n\\n\\nWikibooks (textbooks)\\n\\n\\n\\nWikiquote (quotations)\\n\\n\\n\\nWikisource (library)\\n\\n\\n\\nWikiversity (learning resources)\\n\\n\\n\\nCommons (media)\\n\\n\\n\\nWikivoyage (travel guide)\\n\\n\\n\\nWikinews (news source)\\n\\n\\n\\nWikidata (linked database)\\n\\n\\n\\nWikispecies (species directory)\\n\\n\\n\\nWikipedia does not have an article with this exact name. Please search for Arabic religion in Wikipedia to check for alternative titles or spellings.\\nYou need to log in or create an account and be autoconfirmed to create new articles. Alternatively, you can use the article wizard to submit a draft for review, or request a new article.\\nSearch for \"Arabic religion\" in existing articles.\\nLook for pages within Wikipedia that link to this title.\\n\\n\\nOther reasons this message may be displayed:'), Document(page_content='Ottoman Era (1517 AD – 1805AD)** ** \\n   - Following the fall of the Mamluks, Egypt came \\nunder Ottoman control, experiencing economic and political challenges during centuries of \\nOttoman rule. \\n \\n** Modern History of Egypt**: \\n \\n** . The Muhammad Ali Dynasty (1805 - 1952**:) \\n   - Muhammad Ali Pasha founded modern Egypt \\nby modernizing the military, education, and \\nindustry. His dynasty ruled until the 1952 \\nrevolution . \\n \\n** . The 1952 Revolution and the Republic **: \\n   - Led by Gamal Abdel Nasser, the revolution \\nended the monarchy and established a republic. \\nThis era saw the nationalization of the Suez Canal, \\nthe construction of the Aswan High Dam, and \\nseveral conflicts with Israel . \\n \\nModern Wars \\nThe Suez Crisis 1956  Britain, France, and Israel \\ninvaded Egypt following the nationalization of the \\nSuez Canal. Six-Day War1967  Egypt was defeated by Israel, \\nlosing the Sinai Peninsula \\nYom Kippur War 1973  Egypt launched a successful \\nattack to reclaim Sinai \\n   Peace Treaty 1979  Egypt signed a peace treaty \\nwith Israel, mediated by the U.S., which returned \\nSinai to Egypt . \\n \\nFive Prominent Egyptian Governorates and Their \\nHistories **: \\n \\n** . Cairo **: \\n   - Founded by the Fatimids in 969 AD, Cairo is \\nthe administrative and cultural capital of Egypt. It \\nhosts historical landmarks like Al -Azhar, the \\nCitadel, and the Egyptian Museum . \\n \\n** . Alexandria **: \\n   - Founded by Alexander the Great in 331 BC, \\nAlexandria was famous for its library and'), Document(page_content=\"the administrative and cultural capital of Egypt. It \\nhosts historical landmarks like Al -Azhar, the \\nCitadel, and the Egyptian Museum . \\n \\n** . Alexandria **: \\n   - Founded by Alexander the Great in 331 BC, \\nAlexandria was famous for its library and \\nlighthouse, one of the Seven Wonders of the \\nAncient World. It remained a major commercial \\nand cultural center throughout history .  \\n** . Luxor **: \\n   - Known as the city of the kings and temples, \\nLuxor was the capital of Egypt during the New \\nKingdom. It is home to the Karnak and Luxor \\nTemples and the Valley of the Kings. \\n \\n \\n** . Aswan **: \\n   - Aswan was an important trade center and port \\non the Nile. It was known for its quarries, which \\nsupplied stones for temples and statues . \\n \\n** . Port Said **: \\n   - Established in the 19 th century during the \\nconstruction of the Suez Canal, Port Said became a \\nmajor industrial and commercial city and played a \\nsignificant role during the Suez Crisis . \\n \\n** Sinai Peninsula **: \\n   - Historically a strategic land bridge between \\nAfrica and Asia, Sinai has been a battleground for \\nmany conflicts, including the wars between Egypt and Israel. It is home to St. Catherine's Monastery, \\none of the oldest Christian monasteries in the \\nworld, and is known for its natural beauty and \\nreligious significance . \\n \\nEgypt’s history is a rich tapestry that blends \\nancient and modern civilizations, making it a \\nunique crossroads between the East and West \\nthroughout the ages.\")]\n",
      "[Document(page_content=\"** Ancient History of Egypt **:    \\n \\nPharaonic Era **  ** \\n( 3100 BC – 332 BC  ) \\n   - This era marks the beginning of ancient \\nEgyptian civilization, starting with the unification \\nof Upper and Lower Egypt under King Menes. The \\ncivilization flourished with the construction of the \\npyramids, the development of hieroglyphic \\nwriting, and the reign of great pharaohs like \\nRamses II, Tutankhamun, and Cleopatra VII . \\n \\nPtolemaic Era **  **    \\n(332 BC – 30 BC)      \\n   - It began with the conquest of Egypt by \\nAlexander the Great and continued under the rule \\nof the Ptolemaic dynasty led by Ptolemy I. \\nAlexandria became a major center of culture and \\nlearning, with the famous Library of Alexandria . \\n \\n \\n  \\n \\n** Roman and Byzantine Period    **   \\n  (30  641AD - BC  )    \\n   - After Cleopatra's death, Egypt became a \\nprovince of the Roman Empire. Alexandria \\ncontinued to be a cultural and scientific hub \\nduring  \\n \\n** **Islamic Era \\n(641AD - 1517AD)          \\n \\nThe Islamic conquest of Egypt began with Amr ibn \\nal-As, transforming Egypt into a prosperous Islamic \\nstate. Egypt was ruled by various dynasties, \\nincluding the Tulunids, Ikhshidids, Fatimids, \\nAyyubids, and Mamluks . \\n \\n \\nOttoman Era (1517 AD – 1805AD)** ** \\n   - Following the fall of the Mamluks, Egypt came \\nunder Ottoman control, experiencing economic and political challenges during centuries of \\nOttoman rule. \\n \\n** Modern History of Egypt**: \\n \\n** . The Muhammad Ali Dynasty (1805 - 1952**:)\"), Document(page_content='Ottoman Era (1517 AD – 1805AD)** ** \\n   - Following the fall of the Mamluks, Egypt came \\nunder Ottoman control, experiencing economic and political challenges during centuries of \\nOttoman rule. \\n \\n** Modern History of Egypt**: \\n \\n** . The Muhammad Ali Dynasty (1805 - 1952**:) \\n   - Muhammad Ali Pasha founded modern Egypt \\nby modernizing the military, education, and \\nindustry. His dynasty ruled until the 1952 \\nrevolution . \\n \\n** . The 1952 Revolution and the Republic **: \\n   - Led by Gamal Abdel Nasser, the revolution \\nended the monarchy and established a republic. \\nThis era saw the nationalization of the Suez Canal, \\nthe construction of the Aswan High Dam, and \\nseveral conflicts with Israel . \\n \\nModern Wars \\nThe Suez Crisis 1956  Britain, France, and Israel \\ninvaded Egypt following the nationalization of the \\nSuez Canal. Six-Day War1967  Egypt was defeated by Israel, \\nlosing the Sinai Peninsula \\nYom Kippur War 1973  Egypt launched a successful \\nattack to reclaim Sinai \\n   Peace Treaty 1979  Egypt signed a peace treaty \\nwith Israel, mediated by the U.S., which returned \\nSinai to Egypt . \\n \\nFive Prominent Egyptian Governorates and Their \\nHistories **: \\n \\n** . Cairo **: \\n   - Founded by the Fatimids in 969 AD, Cairo is \\nthe administrative and cultural capital of Egypt. It \\nhosts historical landmarks like Al -Azhar, the \\nCitadel, and the Egyptian Museum . \\n \\n** . Alexandria **: \\n   - Founded by Alexander the Great in 331 BC, \\nAlexandria was famous for its library and'), Document(page_content=\"the administrative and cultural capital of Egypt. It \\nhosts historical landmarks like Al -Azhar, the \\nCitadel, and the Egyptian Museum . \\n \\n** . Alexandria **: \\n   - Founded by Alexander the Great in 331 BC, \\nAlexandria was famous for its library and \\nlighthouse, one of the Seven Wonders of the \\nAncient World. It remained a major commercial \\nand cultural center throughout history .  \\n** . Luxor **: \\n   - Known as the city of the kings and temples, \\nLuxor was the capital of Egypt during the New \\nKingdom. It is home to the Karnak and Luxor \\nTemples and the Valley of the Kings. \\n \\n \\n** . Aswan **: \\n   - Aswan was an important trade center and port \\non the Nile. It was known for its quarries, which \\nsupplied stones for temples and statues . \\n \\n** . Port Said **: \\n   - Established in the 19 th century during the \\nconstruction of the Suez Canal, Port Said became a \\nmajor industrial and commercial city and played a \\nsignificant role during the Suez Crisis . \\n \\n** Sinai Peninsula **: \\n   - Historically a strategic land bridge between \\nAfrica and Asia, Sinai has been a battleground for \\nmany conflicts, including the wars between Egypt and Israel. It is home to St. Catherine's Monastery, \\none of the oldest Christian monasteries in the \\nworld, and is known for its natural beauty and \\nreligious significance . \\n \\nEgypt’s history is a rich tapestry that blends \\nancient and modern civilizations, making it a \\nunique crossroads between the East and West \\nthroughout the ages.\"), Document(page_content='one of the oldest Christian monasteries in the \\nworld, and is known for its natural beauty and \\nreligious significance . \\n \\nEgypt’s history is a rich tapestry that blends \\nancient and modern civilizations, making it a \\nunique crossroads between the East and West \\nthroughout the ages. \\n \\n \\n** Famous Landmarks in Egypt**: \\n \\n** . The Pyramids of Giza and the Sphinx**: \\n   - Located near Cairo, these iconic structures \\nwere built during the Old Kingdom as tombs for \\nthe pharaohs. The Great Pyramid of Giza is one of \\nthe Seven Wonders of the Ancient World and the \\nonly one still in existence. \\n \\n** . Karnak Temple Complex **: \\n   - Situated in Luxor, Karnak is one of the largest \\nreligious complexes ever built, dedicated primarily to the god Amun. It features massive columns, \\nobelisks, and statues . \\n \\n** . The Valley of the Kings **: \\n   - Located on the west bank of the Nile in Luxor, \\nthis valley is the burial site of many New Kingdom \\npharaohs, including the tomb of Tutankhamun . \\n \\n** . Abu Simbel Temples **: \\n   - Carved into a mountainside by Ramses II, \\nthese monumental temples in Aswan are famous \\nfor their colossal statues and intricate carvings . \\n \\n** . The Egyptian Museum **: \\n   - Located in Cairo, the museum houses an \\nextensive collection of ancient Egyptian \\nantiquities, including the treasures of \\nTutankhamun . \\n \\n** . Citadel of Saladin and Muhammad Ali \\nMosque **: \\n   - Situated in Cairo, the citadel was built by')]\n"
     ]
    }
   ],
   "source": [
    "# print(vectordb.similarity_search(\"WHAT IS AI\"))\n",
    "print(vectordb.similarity_search(\"WHAT IS MENTAL HEALTH?\"))\n",
    "print(vectordb.similarity_search(\"WHAT IS history of egypt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 38
    },
    "id": "XzevuuC5yw-Q",
    "outputId": "a4b7cc59-d887-45b6-d847-77b6e064510d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'0.2.11'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvTkFl5foJd8"
   },
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw0w6JB_pGPm"
   },
   "outputs": [],
   "source": [
    "qa_chain=load_LLM_groq(vectordb,\"llama-3.1-70b-versatile\")\n",
    "prompt = '''\n",
    "you are an expert historian who knows about egypt ,palastine and arabic history answer the questions as short as possible/n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ffdd6b0-24b5-4411-8340-e20aefd6cc8d",
    "outputId": "a9cb497b-fd39-4054-c642-74ee0e1d89af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-05 21:07:26.645 INFO    httpx: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palestine's history spans over 5,000 years, with various empires and civilizations ruling the region. Here's a brief overview:\n",
      "\n",
      "* Ancient Period (3000 BC - 636 AD): Canaanites, Philistines, Israelites, and other tribes inhabited the region.\n",
      "* Roman and Byzantine Period (636 AD - 638 AD): Palestine was a Roman province, followed by Byzantine rule.\n",
      "* Islamic Period (638 AD - 1517 AD): Palestine was conquered by Arabs and became a major center of Islamic culture.\n",
      "* Ottoman Period (1517 AD - 1917 AD): Palestine was under Ottoman rule, with a significant Jewish population.\n",
      "* British Mandate (1917 AD - 1948 AD): Palestine was under British control, with the 1917 Balfour Declaration supporting a Jewish homeland.\n",
      "* 1948: Israel declared independence, leading to the displacement of Palestinians (Nakba) and ongoing conflict.\n",
      "\n",
      "This is a condensed version of a complex history.\n"
     ]
    }
   ],
   "source": [
    "# # invoke the qa chain and get a response for user query\n",
    "query = prompt + \"\"\"\n",
    "what is the history of palestine?\n",
    "\"\"\"\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDYqg1ZA0lAv",
    "outputId": "8dc03d3d-b05b-4ccc-f8dc-66f05568d8d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-05 21:16:23.519 INFO    httpx: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لا أملك معلومات كافية عن تاريخ إسرائيل، ولكن يمكنني أن أخبرك أن تاريخ إسرائيل الحديث يبدأ بعد عام 1948، عندما أعلنت إسرائيل استقلالها عن الانتداب البريطاني على فلسطين.\n"
     ]
    }
   ],
   "source": [
    "query = prompt + \"\"\"\n",
    "اخبرني عن اسرئيل\n",
    "\"\"\"\n",
    "response = qa_chain.invoke({\"query\": query})\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BV4ANMgETVhi",
    "outputId": "96d1fc27-fd94-4046-90d2-98096583a478"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py -a\n",
    "import streamlit as st\n",
    "\n",
    "vectordb= Load_vectordb(\"VectorDB\",embeddings)\n",
    "prompt = '''\n",
    "you are an expert historian who knows about egypt ,palastine and arabic history answer the questions as short as possible/n\n",
    "'''\n",
    "# Title of the Streamlit app\n",
    "st.title(\"LLM RAG Chatbot\")\n",
    "\n",
    "# Selection box to choose between OpenAI and Groq models\n",
    "model_option = st.selectbox('Choose a model:', ('OpenAI', 'Groq'))\n",
    "\n",
    "# Input box for user query\n",
    "user_input = st.text_input(\"Ask your question:\")\n",
    "user_input = prompt + user_input\n",
    "# Add OpenAI and Groq API keys to the environment\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key\"\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# Button to submit the query\n",
    "if st.button('Get Answer'):\n",
    "\n",
    "    # Check which model the user selected\n",
    "    if model_option == 'OpenAI':\n",
    "        # Load OpenAI model (for example: GPT-4)\n",
    "        qa_chain = load_LLM_openai(vectordb, model_name=\"gpt-4o\")\n",
    "        response = qa_chain.invoke({\"query\": user_input})\n",
    "        st.write(f\"OpenAI's response: {response['result']}\")\n",
    "\n",
    "    elif model_option == 'Groq':\n",
    "        # Load Groq model\n",
    "        qa_chain=load_LLM_groq(vectordb,\"llama-3.2-11b-text-preview\")\n",
    "        response = qa_chain.invoke({\"query\": user_input})\n",
    "        st.write(f\"Groq's response: {response['result']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LHSROUXk40UO",
    "outputId": "1dbcd986-387c-4636-9e37-940dd4fcfcb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py -a\n",
    "import streamlit as st\n",
    "# Initialize session state to store chat history and API keys\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = []\n",
    "if \"openai_api_key\" not in st.session_state:\n",
    "    st.session_state[\"openai_api_key\"] = \"\"\n",
    "if \"groq_api_key\" not in st.session_state:\n",
    "    st.session_state[\"groq_api_key\"] = \"\"\n",
    "\n",
    "# Sidebar for Model Selection and API Key Input\n",
    "st.sidebar.title(\"Configuration\")\n",
    "\n",
    "# API Key Input Fields\n",
    "st.sidebar.subheader(\"Enter API Keys\")\n",
    "st.session_state[\"openai_api_key\"] = st.sidebar.text_input(\"OpenAI API Key\", type=\"password\")\n",
    "st.session_state[\"groq_api_key\"] = st.sidebar.text_input(\"Groq API Key\", type=\"password\")\n",
    "\n",
    "# Model Selection Dropdown\n",
    "model_option = st.sidebar.selectbox('Choose a model:', ('OpenAI', 'Groq'))\n",
    "\n",
    "# Set API keys in environment variables if provided\n",
    "\n",
    "# Load FAISS vectordb (assuming you have already created the FAISS index)\n",
    "vectordb= Load_vectordb(\"VectorDB\",embeddings)\n",
    "prompt = '''\n",
    "you are an expert historian who knows about egypt ,palastine and arabic history answer the questions as short as possible/n\n",
    "'''\n",
    "\n",
    "# Chatbot Interface: Title and Chat History\n",
    "st.title(\"LLM RAG Chatbot\")\n",
    "\n",
    "# Display chat history\n",
    "st.write(\"### Chat History:\")\n",
    "for message in st.session_state[\"messages\"]:\n",
    "    st.write(message)\n",
    "\n",
    "# Input box for user query\n",
    "user_input = st.text_input(\"You:\", key=\"user_input\")\n",
    "\n",
    "# Button to send the query\n",
    "if st.button('Send'):\n",
    "    # Append user input to chat history\n",
    "    st.session_state[\"messages\"].append(f\"You: {user_input}\")\n",
    "    user_input = prompt + user_input\n",
    "\n",
    "    # Ensure that the appropriate API key is provided\n",
    "    if model_option == 'OpenAI' and not st.session_state[\"openai_api_key\"]:\n",
    "        st.error(\"Please enter your OpenAI API key.\")\n",
    "    elif model_option == 'Groq' and not st.session_state[\"groq_api_key\"]:\n",
    "        st.error(\"Please enter your Groq API key.\")\n",
    "    else:\n",
    "        # Process the query using the selected model\n",
    "        if model_option == 'OpenAI':\n",
    "          if st.session_state[\"openai_api_key\"]:\n",
    "            os.environ[\"OPENAI_API_KEY\"] = st.session_state[\"openai_api_key\"]\n",
    "            qa_chain = load_LLM_openai(vectordb, model_name=\"gpt-4\")\n",
    "            response = qa_chain({\"query\": user_input})\n",
    "            bot_response = f\"OpenAI: {response['answer']}\"\n",
    "\n",
    "        elif model_option == 'Groq':\n",
    "          if st.session_state[\"groq_api_key\"]:\n",
    "            os.environ[\"GROQ_API_KEY\"] = st.session_state[\"groq_api_key\"]\n",
    "            qa_chain=load_LLM_groq(vectordb,\"llama-3.2-90b-text-preview\")\n",
    "            response = qa_chain({\"query\": user_input})\n",
    "            bot_response = f\"Groq: {response['result']}\"\n",
    "\n",
    "        # Append the bot's response to the chat history\n",
    "        st.session_state[\"messages\"].append(bot_response)\n",
    "\n",
    "        # Clear input box and refresh chat\n",
    "        st.experimental_rerun()  # Refresh to display the new message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "553KCYHjb5Jw",
    "outputId": "77bffd21-9bf7-4ab1-e0b5-e6f974a83aee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
      "changed 22 packages in 2s\n",
      "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
      "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K3 packages are looking for funding\n",
      "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
      "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K"
     ]
    }
   ],
   "source": [
    "!npm install -g localtunnel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d6eiZnplTWPf",
    "outputId": "756922a2-e05a-4908-cb0e-b17101e18424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.185.230.111\n"
     ]
    }
   ],
   "source": [
    "!wget -q -O - ipv4.icanhazip.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EHs0mUIeYfsP",
    "outputId": "368dd7d1-1171-4656-912c-1677a425466a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\n",
      "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
      "\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
      "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.185.230.111:8501\u001b[0m\n",
      "\u001b[0m\n",
      "your url is: https://ripe-pandas-wait.loca.lt\n",
      "\u001b[34m  Stopping...\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!streamlit run app.py & npx localtunnel --port 8501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qMrd17UoZH5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MLandDl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
